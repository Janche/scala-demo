package org.example.spark

import org.apache.spark.{Partitioner, SparkContext, SparkConf}

//自定义分区类，需要继承org.apache.spark.Partitioner类
class MyPartitioner(numParts: Int) extends Partitioner {
    //覆盖分区数
    override def numPartitions: Int = numParts

    //覆盖分区号获取函数
    override def getPartition(key: Any): Int = {
        key.toString.toInt % 10
    }
}

object TestPartitioner {
    def main(args: Array[String]) {
        val conf = new SparkConf().setAppName("TestPartitioner")
            .setMaster("local[*]")
        val sc = new SparkContext(conf)
        //模拟5个分区的数据
        val data = sc.parallelize(1 to 10, 5)
        //根据尾号转变为10个分区，分别写到10个文件
        data.map((_, 1)).partitionBy(new MyPartitioner(10))
            .map(_._1)
            .saveAsTextFile("output")
    }
}